{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "   \n",
        "\n",
        "Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical and typically takes on two values (e.g., 0 and 1, yes and no, success and failure). Unlike Linear Regression, which predicts a continuous outcome based on linear relationships between the independent variables and the dependent variable, Logistic Regression predicts the probability that a given input point belongs to a particular category.    \n",
        "\n",
        "The key difference lies in the nature of the output. Linear Regression outputs a continuous value, which can theoretically range from negative to positive infinity. In contrast, Logistic Regression uses the logistic function to constrain the output to a range between 0 and 1, making it suitable for binary outcomes. The logistic function, or sigmoid function, transforms the linear combination of inputs into a probability, which can then be thresholded to classify the input into one of the two categories.    \n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "   \n",
        "\n",
        "The mathematical equation for Logistic Regression can be expressed as:  \n",
        "\n",
        "P(Y=1|X) = sigma(W^T X+b)\n",
        "\n",
        "   \n",
        "\n",
        "where:\n",
        "\n",
        "P(Y=1|X)  is the probability that the output  is 1 given the input features\n",
        ".\n",
        "\n",
        "sigma(W^T X+b)   is the sigmoid function.\n",
        "\n",
        " W is the vector of weights (coefficients) for the input features.\n",
        "\n",
        " T is the bias term.\n",
        "\n",
        " x is the feature vector.    \n",
        "The output of the sigmoid function is interpreted as the probability of the positive class, and the decision boundary is typically set at 0.5.    \n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "   \n",
        "\n",
        "The Sigmoid function is used in Logistic Regression because it maps any real-valued number into the range (0, 1), which is ideal for modeling probabilities. The function has an S-shaped curve, which allows it to output values close to 0 for large negative inputs and values close to 1 for large positive inputs. This characteristic makes it suitable for binary classification tasks, as it can effectively model the probability of the positive class.    \n",
        "\n",
        "Additionally, the sigmoid function has a nice mathematical property: its derivative can be expressed in terms of the function itself, which simplifies the computation of gradients during the optimization process. This is particularly useful when using gradient descent to minimize the cost function.    \n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "   \n",
        "\n",
        "The cost function for Logistic Regression is derived from the likelihood of the observed data given the model parameters. It is commonly expressed as the negative log-likelihood, which can be formulated as:  \n",
        "\n",
        "J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right]\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "where:\n",
        "\n",
        "m  is the number of training examples.\n",
        "\n",
        "y^{(i)} is the actual label for the\n",
        "-th example.\n",
        "\n",
        "(h(x^{(i)})) is the predicted probability for the\n",
        "-th example.    \n",
        "This cost function penalizes incorrect predictions, encouraging the model to adjust its parameters to minimize the error between predicted probabilities and actual labels.    \n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "   \n",
        "\n",
        "Regularization is a technique used to prevent overfitting in machine learning models, including Logistic Regression. Overfitting occurs when a model learns the noise in the training data rather than the underlying pattern, leading to poor generalization on unseen data. Regularization adds a penalty term to the cost function, discouraging overly complex models.    \n",
        "\n",
        "In Logistic Regression, two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization. The regularized cost function can be expressed as:   \n",
        "J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right] + \\lambda R(W)\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "where $ R(W) $ is the regularization term\n",
        " (either $ |W|_1 $ for Lasso or $ |W|_2^2 $ for Ridge), and\n",
        "\n",
        "  $ \\lambda $ is the regularization parameter that controls the strength of the penalty.    \n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "   \n",
        "\n",
        "Lasso (L1) and Ridge (L2) regression are two types of regularization techniques used to prevent overfitting in regression models, including Logistic Regression.    \n",
        "\n",
        "Lasso Regression (L1 Regularization): This technique adds a penalty equal to the absolute value of the coefficients. It can shrink some coefficients to zero, effectively performing variable selection. This is useful when we suspect that many features are irrelevant.    \n",
        "$$ R(W) = |W|1 = \\sum{j=1}^{n} |w_j| $$    \n",
        "\n",
        "Ridge Regression (L2 Regularization): This technique adds a penalty equal to the square of the coefficients. It tends to shrink coefficients but does not set them to zero, which means it retains all features in the model.    \n",
        "$$ R(W) = |W|2^2 = \\sum{j=1}^{n} w_j^2 $$    \n",
        "\n",
        "Elastic Net: This is a combination of Lasso and Ridge regression. It incorporates both L1 and L2 penalties, allowing for both variable selection and coefficient shrinkage. Elastic Net is particularly useful when there are correlations among features.    \n",
        "   \n",
        "\n",
        "where $ \\alpha $ is a mixing parameter between Lasso and Ridge.    \n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "   \n",
        "\n",
        "Elastic Net is particularly useful in scenarios where there are multiple features that are correlated with each other. In such cases, Lasso may arbitrarily select one feature from a group of correlated features while ignoring others, which can lead to instability in the model. Ridge, on the other hand, retains all features but does not perform variable selection.    \n",
        "\n",
        "Elastic Net combines the strengths of both Lasso and Ridge, allowing for both variable selection and regularization. It is recommended to use Elastic Net when:\n",
        "\n",
        "There are many features, and some are highly correlated.\n",
        "You want to perform variable selection while also controlling for multicollinearity.\n",
        "You have a large number of features compared to the number of observations.    \n",
        "8. What is the impact of the regularization parameter (\n",
        ") in Logistic Regression?\n",
        "   \n",
        "\n",
        "The regularization parameter $ \\lambda $ controls the strength of the penalty applied to the coefficients in Logistic Regression. Its impact can be summarized as follows:    \n",
        "\n",
        "When\n",
        ": The model behaves like standard Logistic Regression without regularization. It may fit the training data very well, but it risks overfitting, especially in high-dimensional spaces.    \n",
        "\n",
        "When\n",
        " is small: The model applies a light penalty, allowing for some flexibility in fitting the training data while still providing some regularization to prevent overfitting.    \n",
        "\n",
        "When\n",
        " is large: The penalty becomes significant, leading to more substantial shrinkage of the coefficients. This can result in a simpler model with fewer effective features, which may improve generalization on unseen data but could also lead to underfitting if too many coefficients are driven to zero.    \n",
        "\n",
        "Choosing the right value for $ \\lambda $ is crucial and is often done through techniques like cross-validation.    \n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "   \n",
        "\n",
        "Logistic Regression relies on several key assumptions:    \n",
        "\n",
        "Binary Outcome: The dependent variable should be binary (0 or 1).    \n",
        "\n",
        "Independence of Observations: The observations should be independent of each other. This means that the outcome of one observation should not influence another.    \n",
        "\n",
        "Linearity of Logits: The log-odds of the outcome should be linearly related to the independent variables. This means that while the relationship between the independent variables and the probability of the outcome is nonlinear, the relationship between the independent variables and the log-odds (logit) should be linear.    \n",
        "\n",
        "No Multicollinearity: The independent variables should not be too highly correlated with each other. High multicollinearity can lead to unstable estimates of the coefficients.    \n",
        "\n",
        "Large Sample Size: Logistic Regression requires a sufficiently large sample size to provide reliable estimates. Small sample sizes can lead to overfitting and unreliable predictions.    \n",
        "\n",
        "Absence of Outliers: Outliers can disproportionately influence the model's estimates, leading to biased results. It is important to check for and address outliers in the dataset.    \n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "   \n",
        "\n",
        "There are several alternatives to Logistic Regression for classification tasks, including:    \n",
        "\n",
        "Decision Trees: A non-parametric method that splits the data into subsets based on feature values, creating a tree-like model of decisions.    \n",
        "\n",
        "Random Forest: An ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and control overfitting.    \n",
        "\n",
        "Support Vector Machines (SVM): A method that finds the hyperplane that best separates the classes in the feature space, maximizing the margin between the classes.    \n",
        "\n",
        "K-Nearest Neighbors (KNN): A non-parametric method that classifies a data point based on the majority class of its nearest neighbors in the feature space.    \n",
        "\n",
        "Neural Networks: A flexible and powerful method that can model complex relationships in data through layers of interconnected nodes.    \n",
        "\n",
        "Gradient Boosting Machines (GBM): An ensemble technique that builds models sequentially, where each new model corrects the errors of the previous ones.    \n",
        "\n",
        "Naive Bayes: A probabilistic classifier based on Bayes' theorem, assuming independence among predictors.    \n",
        "\n",
        "Each of these methods has its strengths and weaknesses, and the choice of which to use often depends on the specific characteristics of the dataset and the problem at hand.    \n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "   \n",
        "\n",
        "Classification evaluation metrics are used to assess the performance of classification models. Some common metrics include:    \n",
        "\n",
        "Accuracy: The proportion of correctly classified instances out of the total instances. It is calculated as:    \n",
        "\n",
        "Accuracy = True Positives + True Negatives / Total Instances\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "Precision: The proportion of true positive predictions out of all positive predictions. It measures the accuracy of the positive class predictions:    \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "Recall (Sensitivity): The proportion of true positive predictions out of all actual positive instances. It measures the model's ability to identify positive instances:    \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "F1 Score: The harmonic mean of precision and recall, providing a balance between the two metrics:    \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "ROC-AUC: The Receiver Operating Characteristic curve plots the true positive rate against the false positive rate at various threshold settings. The Area Under the Curve (AUC) quantifies the overall ability of the model to discriminate between classes.    \n",
        "\n",
        "Confusion Matrix: A table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives.    \n",
        "\n",
        "These metrics help in understanding the strengths and weaknesses of a classification model and guide improvements.    \n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "   \n",
        "\n",
        "Class imbalance occurs when the number of instances in one class is significantly higher than in another. This can adversely affect the performance of Logistic Regression and other classification algorithms in several ways:    \n",
        "\n",
        "Bias Toward Majority Class: The model may become biased toward the majority class, leading to high accuracy but poor performance on the minority class. For example, if 95% of the data belongs to class 0 and only 5% to class 1, a model that predicts all instances as class 0 could achieve 95% accuracy while failing to identify any instances of class 1.    \n",
        "\n",
        "Poor Recall for Minority Class: The model may struggle to correctly identify instances of the minority class, resulting in low recall and high false negative rates.    \n",
        "\n",
        "Misleading Evaluation Metrics: Standard evaluation metrics like accuracy can be misleading in imbalanced datasets. Metrics such as precision, recall, and F1 score become more informative in these cases.    \n",
        "\n",
        "To address class imbalance, techniques such as resampling (oversampling the minority class or undersampling the majority class), using different evaluation metrics, or employing specialized algorithms designed to handle imbalanced data can be employed.    \n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "   \n",
        "\n",
        "Hyperparameter tuning refers to the process of optimizing the hyperparameters of a model to improve its performance. In the context of Logistic Regression, hyperparameters include:    \n",
        "\n",
        "Regularization Parameter (\n",
        "): Controls the strength of the regularization applied to the model. Tuning this parameter helps balance the trade-off between bias and variance.    \n",
        "\n",
        "Solver: The algorithm used to optimize the cost function (e.g., 'liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'). Different solvers may perform better depending on the dataset size and characteristics.    \n",
        "\n",
        "Maximum Iterations: The maximum number of iterations for the optimization algorithm. Tuning this can help ensure convergence.    \n",
        "\n",
        "Tolerance: The threshold for stopping criteria. It determines when the optimization process should stop based on the change in the cost function.    \n",
        "\n",
        "Hyperparameter tuning can be performed using techniques such as grid search, random search, or more advanced methods like Bayesian optimization. Cross-validation is often used to evaluate the performance of different hyperparameter settings.    \n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "   \n",
        "\n",
        "Logistic Regression can be optimized using various solvers, each with its strengths and weaknesses. Common solvers include:    \n",
        "\n",
        "liblinear: A good choice for small datasets and supports L1 regularization. It is based on coordinate descent and is efficient for binary classification.    \n",
        "\n",
        "newton-cg: Uses the Newton-Raphson method for optimization. It is suitable for larger datasets and supports L2 regularization.    \n",
        "\n",
        "lbfgs: An optimization algorithm that approximates the Hessian matrix. It is efficient for large datasets and supports both L1 and L2 regularization.    \n",
        "\n",
        "sag: Stochastic Average Gradient descent, which is faster for large datasets. It supports L2 regularization.    \n",
        "\n",
        "saga: An extension of sag that supports both L1 and L2 regularization. It is particularly useful for large datasets with many features.    \n",
        "\n",
        "The choice of solver depends on the size of the dataset, the presence of regularization, and the specific characteristics of the problem. For small datasets, liblinear is often preferred, while for larger datasets, lbfgs or saga may be more efficient.    \n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "   \n",
        "\n",
        "Logistic Regression can be extended to handle multiclass classification problems using two main approaches:    \n",
        "\n",
        "One-vs-Rest (OvR): In this approach, a separate binary classifier is trained for each class. Each classifier predicts the probability of its class versus all other classes. The final prediction is made by selecting the class with the highest predicted probability. This method is straightforward and works well for many applications.    \n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression): This approach generalizes Logistic Regression to multiple classes by using the softmax function. The softmax function outputs a probability distribution across all classes, ensuring that the probabilities sum to 1. The model is trained to maximize the likelihood of the observed data across all classes simultaneously.    \n",
        "\n",
        "The softmax function is defined as:    \n",
        "\n",
        "P(Y=k|X) = \\frac{e^{W_k^T X + b_k}}{\\sum_{j=1}^{K} e^{W_j^T X + b_j}}\n",
        "\n",
        "   \n",
        "\n",
        "where $ K $ is the number of classes, $ W_k $ and $ b_k $ are the weights and bias for class $ k $, respectively.    \n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "   \n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simplicity: Logistic Regression is easy to implement and interpret, making it a good starting point for binary classification tasks.\n",
        "Efficiency: It is computationally efficient and can handle large datasets well.\n",
        "Probabilistic Output: It provides probabilities for class membership, which can be useful for decision-making.\n",
        "Feature Importance: The coefficients can be interpreted to understand the influence of each feature on the outcome.    \n",
        "Disadvantages:\n",
        "\n",
        "Linearity Assumption: Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the outcome, which may not hold in all cases.\n",
        "Sensitivity to Outliers: It can be affected by outliers, which can skew the results.\n",
        "Limited to Binary Outcomes: While it can be extended to multiclass problems, it is inherently designed for binary classification.\n",
        "Multicollinearity Issues: High correlation among independent variables can lead to unstable coefficient estimates.    \n",
        "17. What are some use cases of Logistic Regression?\n",
        "   \n",
        "\n",
        "Logistic Regression is widely used in various fields for binary classification tasks, including:    \n",
        "\n",
        "Medical Diagnosis: Predicting the presence or absence of a disease based on patient features (e.g., predicting whether a patient has diabetes based on medical history and test results).    \n",
        "\n",
        "Credit Scoring: Assessing the likelihood of a borrower defaulting on a loan based on financial history and demographic information.    \n",
        "\n",
        "Marketing: Predicting whether a customer will respond to a marketing campaign based on their characteristics and past behavior.    \n",
        "\n",
        "Spam Detection: Classifying emails as spam or not spam based on features extracted from the email content.    \n",
        "\n",
        "Customer Churn Prediction: Identifying customers who are likely to stop using a service based on their usage patterns and demographics.    \n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "   \n",
        "\n",
        "The primary difference between Softmax Regression and Logistic Regression lies in their application to classification problems:    \n",
        "\n",
        "Logistic Regression: Designed for binary classification tasks, it predicts the probability of one class versus the other. It uses the sigmoid function to output probabilities constrained between 0 and 1.    \n",
        "\n",
        "Softmax Regression: An extension of Logistic Regression for multiclass classification. It predicts probabilities for multiple classes simultaneously, ensuring that the probabilities sum to 1. The softmax function is used to convert the raw scores (logits) for each class into probabilities.    \n",
        "\n",
        "In summary, while Logistic Regression is suitable for binary outcomes, Softmax Regression is used when there are three or more classes.    \n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "   \n",
        "\n",
        "The choice between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors:    \n",
        "\n",
        "Number of Classes: If the number of classes is small, both methods can work well. However, as the number of classes increases, Softmax may be more efficient since it optimizes the probabilities across all classes simultaneously.    \n",
        "\n",
        "Interpretability: OvR can be easier to interpret since it provides separate models for each class. This can be beneficial when understanding the contribution of each feature to each class.    \n",
        "\n",
        "Performance: Softmax can provide better performance in cases where classes are not mutually exclusive or when there are correlations among classes. It captures the relationships between classes more effectively.    \n",
        "\n",
        "Computational Efficiency: Softmax may be more computationally efficient for large datasets with many classes, as it avoids training multiple binary classifiers.    \n",
        "\n",
        "Ultimately, the choice may also depend on empirical performance, and it is advisable to evaluate both approaches on the specific dataset to determine which yields better results.    \n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "   \n",
        "\n",
        "In Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit increase in the corresponding predictor variable, holding all other variables constant. The interpretation can be summarized as follows:    \n",
        "\n",
        "A positive coefficient indicates that as the predictor variable increases, the log-odds of the outcome being 1 (the positive class) also increase. This implies a higher probability of the positive class.    \n",
        "\n",
        "A negative coefficient indicates that as the predictor variable increases, the log-odds of the outcome being 1 decrease, implying a lower probability of the positive class.    \n",
        "\n",
        "To interpret the coefficients in terms of odds, we can exponentiate them:   \n",
        "\n",
        "Odds Ratio = e^{beta}\n",
        "\n",
        "   \n",
        "\n",
        "where $ \\beta_j $ is the coefficient for predictor $ j $. The odds ratio indicates how the odds of the outcome change with a one-unit increase in the predictor variable:    \n",
        "\n",
        "An odds ratio greater than 1 indicates an increase in odds for the positive class.\n",
        "An odds ratio less than 1 indicates a decrease in odds for the positive class.\n",
        "An odds ratio equal to 1 indicates no effect.    \n",
        "This interpretation allows practitioners to understand the influence of each feature on the outcome and make informed decisions based on the model's results.    \n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mZHCbJk2-HQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAVnyo0AvbJJ",
        "outputId": "65d2d795-c5ba-4c85-dcaf-7e4c4cb0119c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracy?\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "#and print the model accuracy?\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Logistic Regression with L1\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularized Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxmbU6YewOhA",
        "outputId": "1e4c103a-2964-4e78-c625-3c3b9821f02d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularized Logistic Regression Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "#LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "# Same setup as above\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L2 Regularized Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGwccqdhwjU6",
        "outputId": "7e4c6e35-cc3c-45fb-cab7-696bc42518eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularized Logistic Regression Accuracy: 0.956140350877193\n",
            "Coefficients: [[ 1.82615278e+00  9.70538186e-02  6.41959627e-02 -6.13058134e-03\n",
            "  -1.31665797e-01 -3.62358111e-01 -5.29827543e-01 -2.77361673e-01\n",
            "  -2.50553584e-01 -2.42466568e-02  2.02968663e-02  9.71481901e-01\n",
            "   1.31323552e-01 -1.07238299e-01 -7.71444794e-03  1.23136167e-03\n",
            "  -3.47202367e-02 -2.92427729e-02 -3.39942202e-02  7.73286885e-03\n",
            "   1.40739583e+00 -3.01282635e-01 -2.28908025e-01 -2.13601796e-02\n",
            "  -2.23746404e-01 -1.13481058e+00 -1.54822520e+00 -5.62982453e-01\n",
            "  -6.43847065e-01 -1.21355161e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Standardization + ElasticNet\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"ElasticNet Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuVE32RIxAV3",
        "outputId": "15a50feb-539c-4b27-ebb1-3a0b661caab9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet Logistic Regression Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "#multi_class='ovr'\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load multiclass dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Logistic Regression with OvR\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass OvR Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWLx4GuaxMMm",
        "outputId": "b2e0a66a-b403-4173-a533-fd5cf1d44db1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass OvR Accuracy: 0.8888888888888888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\"C Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "#Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkidC2w1xXtz",
        "outputId": "eb443fab-0e90-494a-abdc-89a94b0d2c66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9800000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "#average accuracy\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(\"Stratified K-Fold Accuracies:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcaioqFWxnDI",
        "outputId": "f9879043-dafa-47c0-9e9e-a919cb147c85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Accuracies: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
            "Average Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "#accuracy.\n",
        "import pandas as pd\n",
        "\n",
        "# Load from CSV (replace with your own file path)\n",
        "#df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Example: assume last column is the target\n",
        "#X = df.iloc[:, :-1]\n",
        "#y = df.iloc[:, -1]\n",
        "\n",
        "# Train/test split and logistic regression\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#model = LogisticRegression(max_iter=1000)\n",
        "#model.fit(X_train, y_train)\n",
        "\n",
        "#y_pred = model.predict(X_test)\n",
        "print(\"CSV Data Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhOUKrK1xy82",
        "outputId": "b6da38bb-e015-499b-8604-d95a2a6d770c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Data Accuracy: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "#Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_distributions=param_dist, n_iter=5, cv=5, scoring='accuracy')\n",
        "search.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Accuracy:\", search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNYPyImfyS8P",
        "outputId": "e22c8399-e5cb-4cc1-91c2-269f84aa9444"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 1}\n",
            "Best Accuracy: 0.9600000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"One-vs-One Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybrB5qPPyhjl",
        "outputId": "17c38cdc-23c7-4012-99ae-f60906b93839"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "#classification\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8Cog5iAkzB1h",
        "outputId": "77300f6a-15be-4a30-8480-feafc4568466"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "#12Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "#Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score # Uncomment these imports\n",
        "\n",
        "# Fit model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Specify the average parameter for multiclass metrics\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKSkshLg01cz",
        "outputId": "eebc0a52-98f7-413d-82ae-5c35633e661e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9796296296296295\n",
            "Recall: 0.9777777777777777\n",
            "F1 Score: 0.9779434092477569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "#improve model performance\n",
        "\n",
        "\n",
        "# Train with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Balanced Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score (Balanced):\", f1_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxBDH--T09Sq",
        "outputId": "1d091147-fe04-49b0-9f5d-8c6d71e8d414"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Accuracy: 0.7342657342657343\n",
            "F1 Score (Balanced): 0.6935483870967742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "#evaluate performance\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic data\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'fare']].dropna()\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Titanic Dataset Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUI47iG15Kr",
        "outputId": "584b4063-6471-4fbd-b656-8bb4015fb356"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Dataset Accuracy: 0.7552447552447552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "#model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "acc_no_scaling = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSiCw3Vo2H8Y",
        "outputId": "9286792f-a7b3-4433-8781-9a81adc58df6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.7552447552447552\n",
            "Accuracy with Scaling: 0.7412587412587412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4_D3e-S3PpE",
        "outputId": "55e46d5e-4989-4158-b9db-5fbefa271e0e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.8109605911330049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "#accuracy\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Custom C=0.5 Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofid6Jzf3Xu6",
        "outputId": "3a1f3661-5f85-43a4-8cdf-7bc04e791c65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom C=0.5 Accuracy: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train Logistic Regression and identify important features based on model\n",
        "#coefficients\n",
        "feature_names = X.columns if hasattr(X, 'columns') else [f'Feature {i}' for i in range(X.shape[1])]\n",
        "coeffs = model.coef_[0]\n",
        "\n",
        "for feature, coef in zip(feature_names, coeffs):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIYCn4DX3kJR",
        "outputId": "fb606531-4f72-4e7b-f482-9847b775e099"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pclass: -1.1824\n",
            "sex: 2.3990\n",
            "age: -0.0414\n",
            "fare: 0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19Write a Python program to train Logistic Regression and evaluate its performance using Cohens Kappa\n",
        "#Score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print(\"kajal kothari Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7nqGiIh3v9H",
        "outputId": "af543507-9d93-4df3-d269-f47cbf6842c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kajal kothari Score: 0.462831158560696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "#classification:\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uqNnCDsQ4Bvu",
        "outputId": "a5a77357-1585-4f15-f1f5-01cb79c0f1f7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "#their accuracy\n",
        "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"{solver} solver accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKld7nqZ4Sb-",
        "outputId": "ca65ffb6-4d1e-474b-d668-6a0efa9b06c2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liblinear solver accuracy: 0.7692\n",
            "saga solver accuracy: 0.6573\n",
            "lbfgs solver accuracy: 0.7552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "#Correlation Coefficient (MCC)\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-qUubbA4vcx",
        "outputId": "fa9282ae-9571-4954-e956-dafb9d77018f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.46963741166139705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "#accuracy to see the impact of feature scaling\n",
        "# Already shown above (see Task 4), included again for completeness\n",
        "print(\"Raw Accuracy:\", acc_no_scaling)\n",
        "print(\"Standardized Accuracy:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p47y_EnX9ODy",
        "outputId": "cbdce743-1cfe-46cf-a63e-5177cb628a33"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Accuracy: 0.7552447552447552\n",
            "Standardized Accuracy: 0.7412587412587412\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "#cross-validation\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "param_grid = {'C': np.logspace(-3, 3, 7)}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C:\", grid.best_params_['C'])\n",
        "print(\"Cross-Validated Accuracy:\", grid.best_score_)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpNTR16L9nxX",
        "outputId": "a9a0818b-a349-4bfc-88ec-6e4bb9d1f238"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 0.1\n",
            "Cross-Validated Accuracy: 0.8214340198321892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "#make predictions.\n",
        "import joblib\n",
        "\n",
        "# Train and save\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, \"logreg_model.pkl\")\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load(\"logreg_model.pkl\")\n",
        "print(\"Loaded Model Accuracy:\", accuracy_score(y_test, loaded_model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCHZs5t39tAg",
        "outputId": "0dfacf5c-1306-4baf-98b1-512d924ce8b2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Accuracy: 0.7552447552447552\n"
          ]
        }
      ]
    }
  ]
}